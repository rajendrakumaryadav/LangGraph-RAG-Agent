# LangChain RAG

## Overview
LangChain RAG is an advanced Retrieval-Augmented Generation (RAG) system built with LangChain, LangGraph, FastAPI, and Streamlit. It enables users to ingest documents, perform semantic search, and ask questions with answers generated by state-of-the-art language models, all with fact-checking and critique capabilities.

## Features
- **Document Ingestion:** Upload and ingest PDF, DOCX, and TXT files.
- **Semantic Search:** Uses vector embeddings and Qdrant for efficient document retrieval.
- **Retrieval-Augmented Generation:** Combines retrieved context with LLMs to answer user questions.
- **Fact-Checking:** Critiques generated answers for accuracy and reliability.
- **API Access:** FastAPI endpoints for programmatic interaction.
- **Interactive UI:** Streamlit app for document upload and conversational Q&A.

## Architecture
- **core/ingest.py:** Loads and splits documents, embeds chunks, and stores them in Qdrant.
- **core/rag.py:** Implements the RAG workflow using LangChain and LangGraph, including retrieval, answer generation, and critique.
- **core/models.py:** Provides access to generator, critic, and embedding models.
- **core/retriever.py:** Creates retrievers for semantic search.
- **core/stores.py:** Manages vector store operations.
- **apps/fastapi-apps.py:** FastAPI app exposing `/ask` endpoint for Q&A.
- **apps/streamlit-app.py:** Streamlit app for document ingestion and chat-based Q&A.

## Installation
1. **Clone the repository:**
   ```bash
   git clone <repo-url>
   cd langchain-rag
   ```
2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   # or use poetry if pyproject.toml is present
   poetry install
   ```
3. **Start Qdrant (Vector Store):**
   - Use Docker Compose:
     ```bash
     docker-compose up -d
     ```
   - Or install Qdrant locally: https://qdrant.tech/

## Usage
### Streamlit UI
Run the Streamlit app for interactive document upload and Q&A:
```bash
streamlit run apps/streamlit-app.py
```
- Upload documents in the sidebar.
- Ask questions in the chat interface.

### FastAPI
Run the FastAPI app for API access:
```bash
uvicorn apps.fastapi-apps:app --reload
```
- **POST /ask**: Submit a question and receive an answer.
  - Request: `{ "question": "Your question here" }`
  - Response: `{ "answer": "Generated answer" }`

### Docker Compose
To run all services together:
```bash
docker-compose up --build
```

## Technical Details
- **Document Ingestion:** Uses DoclingLoader for robust file parsing and LangChain's text splitter for chunking.
- **Embeddings:** Supports pluggable embedding models (see `core/models.py`).
- **Vector Store:** Qdrant is used for fast, scalable semantic search.
- **RAG Workflow:** LangGraph orchestrates retrieval, answer generation, and critique nodes.
- **Extensibility:** Easily add new models, retrievers, or storage backends.

## Contributing
1. Fork the repository.
2. Create a new branch for your feature or bugfix.
3. Submit a pull request with a clear description of your changes.
4. Ensure code is well-documented and tested.

## License
Specify your license here (e.g., MIT, Apache 2.0).

## Acknowledgements
- [LangChain](https://github.com/langchain-ai/langchain)
- [Qdrant](https://qdrant.tech/)
- [Streamlit](https://streamlit.io/)
- [FastAPI](https://fastapi.tiangolo.com/)

---
For more details, see the source code and comments in each module.

